# 分页爬取的弊端

```
prepare begin==================================================
当前已经抵达爬虫页数的尽头 16583 16583 829120
```

查看数据库稿件数：827568。而b站API此时给出总稿件数 829120。大约相差1600左右。

此时，没有页码可以继续爬取了。但是，因为时间推移的关系，如果回到开头第一页，会发现这些数据都是没有被爬取的新数据。

**于是，我们需要重新从第1页开始爬取，一直爬取到哪一页呢？**

## 分析

使用图解来说明，注意之前的ps=50恒定：

```
页数递增方向---->
=================================================================
NNNNNNNNNNN*******************************************************************
```

- `=`这一行表示刚开始爬虫时稿件分布。此时的时间点记为T1。
- 第二行表示爬虫完成：N表示新稿件，`*`表示已被爬取的稿件。此时的时间点记为T2。

T1的值可以从数据表mad_crawler_page中create_time获取，或者扫描mad表，找出稿件最大pubdate值（时间戳形式，需要转化）。

我们的目标是为了确定这些N究竟占了多少页。再具体一点，我们想知道T1-T2这段时间里，新的MAD稿件有多少？

由于新稿件数是个真随机数。所以，这里先给出第一种计算方式：基于最近一段时间内的每日MAX投稿数。

## 返回第一页爬取新数据

假设T1-T2之间时间范围是1.6天，向上取整为2天。但是，现在我们不知道每日MAX投稿数是多少。

所以，有必要考察最近一段时间内的每天投稿数：
> 请注意：
> - 这里不能使用日均投稿数，因为要保证高容错抓取，取最大值才足够包含。
> - 也不能使用多年前的时间范围，因为T1-T2这段时间是相对于现在而言，假如你拿10年前的投稿趋势，显然不合理。

基于下面的API例子，修改time_from和time_to的值。考察每天的稿件数。

```bash
https://s.search.bilibili.com/cate/search?main_ver=v3&search_type=video&view_type=hot_rank&order=click&copy_right=-1
&cate_id=24&page=1&pagesize=20&jsonp=jsonp&time_from=20211106&time_to=20211106
```

结果如下表：

|日期|总稿件数|
|---|---|
|20211106|1099|
|20211105|688|
|20211104|563|
|20211103|506|
|20211102|537|
|20211101|612|
|20211031|1026|

取每日MAX投稿数为1500，这样可以计算为：

```
猜测新增稿件数 = 时间范围天数取整 * 日均MAX投稿数 = 2* 1500=3000
```

那么需要爬取页数为：3000/50=60 页。

## 实现

将之前crawl中抓取逻辑抽取为一个函数，这个函数爬取指定页数范围的数据。

``` python
def crawl_by_range(page_start, page_end, ps):
   # ...
```

现在执行：crawl_by_range(1, 60, 50)。

最终mongodb爬取结果为：828991 ，而此时b站API总稿件数为 829160。

？？？

这100多个稿件为什么没有抓取到？发生甚么事了？

原因: 早期爬虫代码不完善。发生了这种情况，

- upsert_to_db(data) 失败
- upsert_mad_crawler_page() 成功

此时依旧走到了下一页的爬取。因此略过了部分页码数据的抓取。

## 小结

- 分页爬取的弊端很明显，容易受时间偏移影响，导致需要返回第1页继续爬取一段距离，来补充这段时间偏移内的新数据。
- 而且，分页爬取存在致命弱点，一旦发生意外中断，人工审查数据是否正确几乎不可能。而按时间范围审查数据是否正确非常简单。
- 但是，后面会指出：分页爬取的数据信息比较全面，按时间范围爬取的数据信息很少。